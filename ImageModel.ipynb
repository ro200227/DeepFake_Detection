{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc10869-70d2-422c-b6e7-e5c8f8ef4747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 10:38:59.436377: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-05 10:39:02.175086: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-05 10:39:02.178867: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-05 10:39:08.275125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === Configuration ===\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "BASE_DIR = \"home/chandana/Documents/deepfake_detection_project/deep1\"\n",
    "\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"Test\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"Validation\")\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"Train\")\n",
    "\n",
    "# === Data Generators ===\n",
    "datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "test_gen = datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=1,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# === Build CNN Model ===\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Train the model ===\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS)\n",
    "\n",
    "# === Predict on Test Set ===\n",
    "test_gen.reset()\n",
    "preds = (model.predict(test_gen) > 0.5).astype(\"int32\")\n",
    "\n",
    "# === Classification Report ===\n",
    "print(\"\\nðŸ“Š Classification Report on Test Data:\")\n",
    "print(classification_report(test_gen.classes, preds, target_names=['Real', 'Fake']))\n",
    "\n",
    "# === Save Model ===\n",
    "# Save architecture\n",
    "model_json = model.to_json()\n",
    "with open(\"cnn_architecture.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save weights\n",
    "model.save_weights(\"cnn_weights.h5\")\n",
    "\n",
    "# Save model info using pickle\n",
    "with open(\"cnn_model_pickle.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"architecture_file\": \"cnn_architecture.json\",\n",
    "        \"weights_file\": \"cnn_weights.h5\"\n",
    "    }, f)\n",
    "\n",
    "print(\"\\nâœ… Model architecture and weights saved using Pickle-style.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c880020-0a05-4d0f-ba87-154541c2ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === Configuration ===\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "BASE_DIR = \"home/chandana/Documents/deepfake_detection_project/deep1\"\n",
    "\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"test\")\n",
    "#VAL_DIR = os.path.join(BASE_DIR, \"Validation\")\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe0260c6-36f2-490d-b1bd-ea7da331075c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'home/chandana/Documents/deepfake_detection_project/deep1/Test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# === Data Generators ===\u001b[39;00m\n\u001b[1;32m      2\u001b[0m datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m train_gen \u001b[38;5;241m=\u001b[39m \u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m val_gen \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     12\u001b[0m     VAL_DIR,\n\u001b[1;32m     13\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(IMG_SIZE, IMG_SIZE),\n\u001b[1;32m     14\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     15\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m test_gen \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     19\u001b[0m     TEST_DIR,\n\u001b[1;32m     20\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(IMG_SIZE, IMG_SIZE),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/preprocessing/image.py:1648\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1564\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1578\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1579\u001b[0m ):\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m \n\u001b[1;32m   1582\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;124;03m            and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/preprocessing/image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m    562\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    565\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'home/chandana/Documents/deepfake_detection_project/deep1/Test'"
     ]
    }
   ],
   "source": [
    "# === Data Generators ===\n",
    "datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "#val_gen = datagen.flow_from_directory(\n",
    "#    VAL_DIR,\n",
    "#    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "test_gen = datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=1,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27523ed-2825-42c3-ab2e-82a8602da8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === Configuration ===\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "BASE_DIR = \"/home/chandana/Documents/deepfake_detection_project/deep1\"\n",
    "\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"test\")     # <-- Training set\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"train\")     # <-- Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5f2192-9463-4c7d-8b3d-76799d177e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 576 images belonging to 2 classes.\n",
      "Found 286 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "test_gen = datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=1,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5c28a9-a2d7-4e9a-8759-0c33573d1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d93c600-a18f-46b3-bb10-788dcac5228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 11:09:59.910050: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 65028096 exceeds 10% of free system memory.\n",
      "2025-04-05 11:10:00.551845: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 30482432 exceeds 10% of free system memory.\n",
      "2025-04-05 11:10:00.991103: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 23482368 exceeds 10% of free system memory.\n",
      "2025-04-05 11:10:01.282305: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 30482432 exceeds 10% of free system memory.\n",
      "2025-04-05 11:10:01.358808: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25719552 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 21s 985ms/step - loss: 0.7325 - accuracy: 0.4913\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 16s 863ms/step - loss: 0.6880 - accuracy: 0.5660\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 16s 864ms/step - loss: 0.6720 - accuracy: 0.5920\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 16s 893ms/step - loss: 0.6301 - accuracy: 0.6545\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 16s 907ms/step - loss: 0.5930 - accuracy: 0.7083\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 17s 971ms/step - loss: 0.5047 - accuracy: 0.7639\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 16s 880ms/step - loss: 0.4507 - accuracy: 0.7899\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 20s 1s/step - loss: 0.3683 - accuracy: 0.8472\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 17s 919ms/step - loss: 0.2927 - accuracy: 0.8681\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.2157 - accuracy: 0.9201\n"
     ]
    }
   ],
   "source": [
    "# === Train the model ===\n",
    "history = model.fit(train_gen, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7c8481c-7b48-401e-bb3b-655292692bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286/286 [==============================] - 5s 17ms/step\n",
      "\n",
      "ðŸ“Š Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.89      0.88      0.88       209\n",
      "        Fake       0.68      0.70      0.69        77\n",
      "\n",
      "    accuracy                           0.83       286\n",
      "   macro avg       0.78      0.79      0.78       286\n",
      "weighted avg       0.83      0.83      0.83       286\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Predict on Test Set ===\n",
    "test_gen.reset()\n",
    "preds = (model.predict(test_gen) > 0.5).astype(\"int32\")\n",
    "\n",
    "# === Classification Report ===\n",
    "print(\"\\nðŸ“Š Classification Report on Test Data:\")\n",
    "print(classification_report(test_gen.classes, preds, target_names=['Real', 'Fake']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28541e06-dce6-46a7-9d49-a9c6c1d25d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model trained and saved successfully without validation!\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"cnn_architecture.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save weights\n",
    "model.save_weights(\"cnn_weights.h5\")\n",
    "\n",
    "# Save using Pickle-style reference\n",
    "with open(\"cnn_model_pickle.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"architecture_file\": \"cnn_architecture.json\",\n",
    "        \"weights_file\": \"cnn_weights.h5\"\n",
    "    }, f)\n",
    "\n",
    "print(\"\\nâœ… Model trained and saved successfully without validation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b298943-1177-439d-923a-b692d16ac349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved as deepfake_model1.h5\n"
     ]
    }
   ],
   "source": [
    "model.save(\"deepfake_model1.h5\")  # Saves the full model in HDF5 format\n",
    "print(\"âœ… Model saved as deepfake_model1.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c936e716-c48a-4495-8598-cba0595704b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully from JSON and weights.\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "ðŸ§  Prediction: REAL (0.08)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# === Load Model Architecture ===\n",
    "with open(\"/home/chandana/Documents/deepfake_detection_project/cnn_architecture.json\", \"r\") as json_file:\n",
    "    model_json = json_file.read()\n",
    "\n",
    "model = model_from_json(model_json)\n",
    "\n",
    "# === Load Model Weights ===\n",
    "model.load_weights(\"/home/chandana/Documents/deepfake_detection_project/cnn_weights.h5\")\n",
    "print(\"âœ… Model loaded successfully from JSON and weights.\")\n",
    "\n",
    "# === Compile (required before prediction) ===\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Predict a Single Image ===\n",
    "IMG_SIZE = 128  # same as training\n",
    "\n",
    "img_path = \"/home/chandana/Downloads/IMG_20241024_103018(1).jpg\" # <- change this\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "img_array = img_array / 255.0  # Normalize\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(img_array)[0][0]\n",
    "\n",
    "# Show result\n",
    "if prediction > 0.6:\n",
    "    label = \"FAKE\"\n",
    "elif prediction < 0.4:\n",
    "    label = \"REAL\"\n",
    "else:\n",
    "    label = \"UNCERTAIN\"\n",
    "print(f\"ðŸ§  Prediction: {label} ({prediction:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f88478-296a-4afc-921b-cf6f6967d522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
